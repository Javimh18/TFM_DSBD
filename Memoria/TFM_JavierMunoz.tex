% Generated by GrindEQ Word-to-LaTeX 
\documentclass{article} % use \documentstyle for old LaTeX compilers

\usepackage[utf8]{inputenc} % 'cp1252'-Western, 'cp1251'-Cyrillic, etc.
\usepackage[english]{babel} % 'french', 'german', 'spanish', 'danish', etc.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{txfonts}
\usepackage{mathdots}
\usepackage[classicReIm]{kpfonts}
\usepackage{graphicx}

% You can include more LaTeX packages here 


\begin{document}

%\selectlanguage{english} % remove comment delimiter ('%') and select language if required


\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 
\[6\] 

{\bf }

\noindent 
{\bf }

\noindent 
{\bf }

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \textbf{UNIVERSIDAD AUTONOMA DE MADRID}

\noindent 

\noindent \textbf{ESCUELA POLITECNICA SUPERIOR}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{\includegraphics*[width=1.06in, height=0.93in]{image1}        \includegraphics*[width=1.65in, height=1.00in]{image2}       }

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{M\'{a}ster en Big Data y Data Science: ciencia e ingenier\'{i}a de datos}

\noindent \textbf{}

\noindent \textbf{TRABAJO FIN DE M\'{A}STER}

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \textbf{Interpretaci\'{o}n de la lengua de signos mediante el procesamiento de imagen/v\'{i}deo y su exploraci\'{o}n para aplicaciones en el mundo real}

\noindent 

\noindent 

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{Javier Mu\~{n}oz Haro}

\noindent \textbf{Tutor: Juan Carlos San Miguel Avedillo}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{Junio 2023}

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \textbf{Interpretaci\'{o}n de la lengua de signos mediante el procesamiento de imagen/v\'{i}deo y su exploraci\'{o}n para aplicaciones en el mundo real}

\noindent 

\noindent 

\noindent \textbf{AUTOR: Javier Mu\~{n}oz Haro}

\noindent \textbf{TUTOR: Juan Carlos San Miguel Avedillo}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{Escuela Polit\'{e}cnica Superior}

\noindent \textbf{Universidad Aut\'{o}noma de Madrid}

\noindent \textbf{Junio de 2021}

\noindent 

\noindent \textbf{\eject Resumen }

\noindent Todav\'{i}a se debate en qu\'{e} momento de la historia los seres humanos adquirimos la capacidad de comunicarse. El rango de tiempo es muy amplio, ya que existen indicios que datan de hace 2 millones y a\~{n}os y otros de hace solo 50.000 a\~{n}os. Si bien es cierto que la manera m\'{a}s efectiva de comunicaci\'{o}n es la verbal, los humanos, al igual que otras especies, tenemos la capacidad de comunicarnos de manera no verbal. Gracias a esto, personas sin la capacidad de comunicaci\'{o}n verbal, pueden transmitir mensajes los cuales pueden ser comprendidos por cualquier persona, sin la necesidad de signos ling\"{u}\'{i}sticos orales.

\noindent Uno de estos ejemplos es el caso de las personas sordas, mudas o sordomudas. Dada esta discapacidad, utilizan el lenguaje de signos para poder comunicarse con otras personas. La motivaci\'{o}n de este trabajo es poder acercar la lengua de signos a aquellas personas que no la conocen, usando el campo del Deep Learning. 

\noindent El trabajo en el que ha consistido este TFM se basa en explorar las soluciones que existen actualmente para el reconocimiento de acciones y aplicarlas en el reconocimiento de los distintos gestos que componen la lengua de signos. Esto es un reto algo complicado, ya que no solo los modelos est\'{a}n limitados por la cantidad de signos que pueden reconocer, si no que existen muchas lenguas de signos, (tantas como pa\'{i}ses/comunidades existen en el mundo) por lo que hacer un modelo universal, capaz de entender cualquier signo en cualquier lengua se antoja m\'{a}s que complicado. Tambi\'{e}n se ha de mencionar que los conjuntos de datos no son muy extensos, ni muy completos, lo cual ha dificultado mucho la tarea de crear un primer reconocedor de gestos.

\noindent Una vez que se recopilaron los datos, se han dise\~{n}ado varios modelos para poder realizar las predicciones, y se han comparado los resultados para discernir cuales son los m\'{a}s efectivos, de los menos. Los distintos modelos est\'{a}n basados en arquitecturas como las ConvLSTM o 3D CNN, o se han apoyado en herramientas de predicciones de hol\'{i}sticas, como Mediapipe, que se han encargado de extraer las caracter\'{i}sticas m\'{a}s representativas de los signos para poder entrenar al modelo que realiza las predicciones. 

\noindent Posteriormente a las comparaciones, se har\'{a} un estudio de precisi\'{o}n de las predicciones en distintos subconjuntos de gestos, para analizar cu\'{a}l de los modelos generaliza mejor cuando el conjunto de signos a predecir es mayor.

\noindent Finalmente, y con el prop\'{o}sito de poder analizar si con la tecnolog\'{i}a existente, se pueden crear herramientas que traduzcan de manera instant\'{a}nea los signos, se har\'{a}n pruebas que midan el tiempo de reacci\'{o}n de los modelos hasta que estos realizan la predicci\'{o}n, para evaluar si es factible su utilizaci\'{o}n como traductores en tiempo real. 

\noindent \textbf{\eject \eject  \textit{Agradecimientos}}

\noindent \textit{A mi familia por su paciencia, a mis amigos por su apoyo y a Sheila y a Celeste Prieto por introducirme al mundo de la lengua y el habla.}

\noindent \textit{}

\noindent \textit{}

\noindent \textbf{\textit{\eject }}

\textbf{ }

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent 

\noindent 
{\bf ii}

\noindent 

\noindent 
{\bf i}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{INDICE DE CONTENIDOS}

\noindent 1 Introducci\'{o}n 11.1 Motivaci\'{o}n 11.2 Objetivos 11.3 Organizaci\'{o}n de la memoria 12 Estado del arte 32.1 Subsecci\'{o}n 32.1.1 Subsubsecci\'{o}n 33 Dise\~{n}o 53.1 Subsecci\'{o}n 53.1.1 Subsubsecci\'{o}n 54 Desarrollo 74.1 Subsecci\'{o}n 74.1.1 Subsubsecci\'{o}n 75 Integraci\'{o}n, pruebas y resultados 96 Conclusiones y trabajo futuro 96.1 Conclusiones 96.2 Trabajo futuro 9Referencias 11Glosario 13Anexos IAnexo A. Manual de instalaci\'{o}n IAnexo B. Manual del programador IIIAnexo C. Anexo {\dots} V\textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{}

\noindent \textbf{INDICE DE FIGURAS}

\noindent \textbf{}

\noindent Figura 2-1: Logo EPS 3\textbf{}

\noindent \textbf{INDICE DE TABLAS}

\noindent \textbf{}

\noindent \textbf{}
\[9\] 

{\bf }


\section{ Introducci\'{o}n}


\subsection{ Motivaci\'{o}n}

\noindent La comunicaci\'{o}n es esencial de los seres humanos. Es el m\'{e}todo que tenemos para, como especie, transmitir informaci\'{o}n, ideas o datos. La comunicaci\'{o}n, de manera gen\'{e}rica, consta de tres actores principales: emisor, que es el encargado de crear el mensaje y mostrar la intenci\'{o}n de transmitirlo, el mensaje, que contiene la informaci\'{o}n a transmitir y el receptor, que es quien recibe la informaci\'{o}n contenida en el mensaje. Dentro de la comunicaci\'{o}n, existen dos subconjuntos principales, la comunicaci\'{o}n verbal y la comunicaci\'{o}n no verbal. La comunicaci\'{o}n verbal usa signos ling\"{u}\'{i}sticos en su mensaje y tiene dos maneras de transmitirse: de manera oral y de manera escrita. La conjunci\'{o}n de ambas es lo que se conoce como ``habla''. Por otro lado, la comunicaci\'{o}n ``no verbal'' es precisamente el tipo de comunicaci\'{o}n que se realiza sin hacer uso del habla. Algunos ejemplos de esto son el contacto visual, expresiones faciales o gestos.

\noindent La primera forma en la que el ser humano conoce la comunicaci\'{o}n verbal es mediante el sentido auditivo. Cuando cognitivamente este se desarrolla, se ha de ver expuesto a alguna lengua para poder aprenderla, pero si el sentido auditivo no est\'{a} desarrollado o se tiene cualquier tipo de discapacidad asociada que impida la escucha, ser\'{a} muy complicado aprender a comunicarse de manera verbal. Precisamente este impedimento de aprender de una manera oral cualquier lengua est\'{a} asociado a las dificultades en la comunicaci\'{o}n que pueden padecer las personas sordas, mudas y sordomudas.  Se ha de aclarar que no todas las personas que sufren estos tipos de discapacidad son incapaces de comunicarse de manera oral, pero, aquellas que padecen de sordera prelocutiva (aquellas que perdieron la capacidad auditiva de manera total antes de la adquisici\'{o}n o aprendizaje del lenguaje) tendr\'{a}n mucha m\'{a}s dificultad a la hora de comunicarse oralmente que aquellas que pod\'{i}an escuchar antes de desarrollar la discapacidad. De esta incapacidad para comunicarse de manera verbal, nace la lengua de signos.

\noindent El origen de la lengua de signos var\'{i}a enormemente en funci\'{o}n del pa\'{i}s o regi\'{o}n donde nos encontremos. En Espa\~{n}a, por ejemplo, data del siglo XVI, cuando los monjes en los monasterios estaban obligados a guardar silencio. Este impedimento para comunicarse de manera verbal impuls\'{o} que empezasen a comunicarse con signos manuales. Estaban, sin saberlo, comenzando a desarrollar la Lengua de Signos Espa\~{n}ola (LSE). Es interesante saber que, pese a que en Espa\~{n}a se tiene una lengua oficial principal, el castellano, se tienen una variedad lenguas oficiales como el catal\'{a}n, valenciano, gallego o euskera. Pues bien, para cada una de estas lenguas, existe una lengua de signos asociada, como la Lengua de Signos Catalana (LSC) o la lengua de signos valenciana (LSCV). Hasta la comunidad de Andaluc\'{i}a, cuya lengua principal es el castellano, tiene variantes con respecto a la LSE. Esto es aplicable al resto de pa\'{i}ses, donde, sin ir m\'{a}s lejos, la lengua de signos de Gran Breta\~{n}a, Estados Unidos y Australia es distinta, pese a que todos estos pa\'{i}ses tiene como lengua oficial principal el ingl\'{e}s.

\noindent Esto por tanto nos introduce la primera dificultad a la hora de poder crear un traductor de lengua de se\~{n}as, y es que no existe una lengua de se\~{n}as universal, por lo que deber\'{i}amos tener en cuenta en qu\'{e} lengua de signos se est\'{a} comunicando el emisor del mensaje.

\noindent Por otro lado, existe otra dificultad, y es la gran cantidad de gestos que contiene cada una de las lenguas de signos. En sematos(ref https://www.sematos.eu/), el portal europeo de lengua de signos, se tienen registrados un total de 6076 palabras para LSE, 3605 en Lengua de Signos Francesa (LSF) y 297 en LSC. Como se puede ver, solo en estas 3 lenguas, se tienen un total de 9978 gestos diferentes a identificar en 3 lenguas distintas lo cual har\'{i}a de esto un problema multiclase multietiqueta muy complejo.

\noindent La motivaci\'{o}n, por tanto, de este trabajo ser\'{a} comprobar si hoy en d\'{i}a existen t\'{e}cnicas y dise\~{n}os que permitan la viabilidad de, en un futuro, poder tener un traductor de lengua de signos universal que permita traducir cualquier mensaje de lengua de signos de cualquier regi\'{o}n del planeta al mismo mensaje en otra lengua o su traducci\'{o}n a lenguaje verbal, ya sea de manera oral o escrita. El estudio de la instantaneidad a la hora de realizar las predicciones ser\'{a} otro factor importante, ya que un int\'{e}rprete de lengua de signos es capaz de traducir de lenguaje verbal a no verbal en cuesti\'{o}n de fracciones de segundo, por lo que, si queremos que se desarrolle un traductor de lengua de signos a 

Referencia [1] 


\subsection{  Objetivos}

\noindent El objetivo principal es crear un clasificador que realice reconocimiento de acciones para predecir los gestos del lenguaje de signos. El reconocimiento correcto de signos se comprobar\'{a} de dos formas:

\noindent 

\begin{enumerate}
\item  Reconocimiento de videos que el algoritmo no ha visto hasta el momento.

\item  Reconocimiento de acciones en tiempo real mediante OpenCV
\end{enumerate}

\noindent 

\noindent El motivo por el que comprobaremos el correcto funcionamiento del modelo de estas dos formas es debido a que las diferencias entre clasificar un v\'{i}deo del conjunto de test no es lo mismo que realizar predicciones en tiempo real, esto se explicar\'{a} en profundidad m\'{a}s adelante.

\noindent 

\noindent Para cumplir estos objetivos, se necesitar\'{a} principalmente comprender cuales son los algoritmos que mejor funcionan a la hora de reconocer acciones, como preprocesar los datos para que dicho algoritmo aprenda y finalmente evaluar correctamente los resultados obtenidos, sean estos positivos o negativo


\subsection{  Organizaci\'{o}n de la memoria}

\noindent La memoria consta de los siguientes cap\'{i}tulos:

\begin{enumerate}
\item  \textbf{Estudio del estado del arte:} El reconocimiento de acciones y el reconocimiento de lengua de se\~{n}as son campos explorados dentro del mundo del Deep Learning. En esta primera secci\'{o}n explicaremos cuales son los m\'{e}todos y algoritmos m\'{a}s efectivos encontrados hasta la fecha en esta disciplina, y como estos se van a utilizar como punto de apoyo para resolver el problema.

\item  \textbf{Dise\~{n}os de la soluci\'{o}n:} Para poder elaborar una soluci\'{o}n que funcione debidamente, se han implementado varias soluciones para aplicarlas a nuestro problema. Explicaremos en profundidad los fundamentos de dichas soluciones y como se han ajustado para poder resolver el problema.

\item  \textbf{Desarrollo}: Una vez que se han explorado distintas soluciones para nuestro problema, comenzaremos con la explicaci\'{o}n de c\'{o}mo se implementaron las mismas. Se explicar\'{a}n los inconvenientes encontrados y como estos se subsanaron. 

\item  \textbf{Integraci\'{o}n, pruebas y resultados}: Ya con los distintos dise\~{n}os implementados, procedemos a probar como estos funcionan cuando se ven ante nuevos datos. Hay que recordar que, para que un modelo funcione correctamente, este no solo tiene que ser \'{o}ptimo a la hora de reconocer los datos que se han usado para entrenarlo, si no que, cuando se vea expuesto a datos que nunca ha visto, sea capaz de inferir apropiadamente el resultado correcto. Durante la elaboraci\'{o}n de este TFM, esta ha sido la fase donde m\'{a}s tiempo se ha pasado, ya que se han realizado diversas pruebas para poder verificar cual de todas es la soluci\'{o}n m\'{a}s eficiente, en base a los objetivos marcados.

\item  \textbf{Conclusiones y trabajo futuro}: Una vez visto c\'{o}mo se comportan los distintos modelos desarrollados, los compararemos entre ellos y comprobaremos cuales son las fortalezas y flaquezas de cada uno de ellos. Posteriormente, se explorar\'{a}n nuevas v\'{i}as de desarrollo para poder mejorar los resultados o ampliar la funcionalidad de los modelos para poder resolver problemas m\'{a}s complejos.
\end{enumerate}

\noindent \textbf{}

\noindent 


\section{  Estado del arte}


\subsection{ Reconocimiento de acciones}

\noindent El reconocimiento de acciones (\textit{action recognition}) es un campo dentro del Deep Learning que se encarga automatizar la clasificaci\'{o}n de distintas acciones que realizan (usualmente) los humanos. Normalmente, para poder resolver este tipo de problemas, se hace uso de datos basados en v\'{i}deos, donde dichas acciones a reconocer se ven representadas por un sujeto que las realiza. Sus aplicaciones son variadas, desde sistemas de vigilancia, monitorizaci\'{o}n para la medicina o sistemas aut\'{o}nomos de conducci\'{o}n. Actualmente existen varios modelos que se han desarrollado para poder realizar reconocimiento de acciones, como redes neuronales convolucionales o redes neuronales recurrentes y sus potenciales combinaciones.

\noindent 

\noindent Dentro del campo de reconocimiento de lengua de signos, existen diversos modelos que se est\'{a}n utilizando para poder atajar este problema. Los m\'{a}s implementados son las redes neuronales convolucionales 3D (3D-CNNs)(ref), las redes convolucionales basadas en grafos (GCN)(ref) o las redes convolucionales LSTM(ref) (ConvLSTM)(ref) entre otros.


\paragraph{ Estimadores de poses}

\noindent Actualmente existen varias librer\'{i}as o frameworks que realizan el trabajo de estimaci\'{o}n de poses por nosotros. Quiz\'{a}s los m\'{a}s conocidos sean Openpose(ref) y Mediapipe(ref). 

\noindent 

\noindent Openpose, es un sistema de visi\'{o}n por computador que realiza estimaciones de poses humanas en tiempo real. Para ello utiliza una red neuronal convolucional profunda, que analiza la imagen estimando primero las personas que aparecen el frame y posteriormente infiriendo los puntos clave o ``keypoints'' de la pose de los sujetos, este tipo de inferencia es conocida como bottom-up. Es una herramienta donde la velocidad de inferencia es uno de sus puntos clave, ya que es capaz de detectar poses en fracciones de segundo para frames donde existen varios sujetos, cosa que, hasta la fecha, no se hab\'{i}a conseguido, ya que las redes basadas en ``bottom-up'' tardaban hasta minutos en analizar un \'{u}nico frame y extraer las poses. 

\noindent 

\noindent \includegraphics*[width=4.15in, height=2.76in]{image3}

  

 (Figura en el \'{i}ndice m\'{a}s referencia: http://naoki.io/portfolio/person\_descrip.html)

\noindent Mediapipe tambi\'{e}n es un sistema de visi\'{o}n por computador, desarrollado por el equipo de Google Research para la estimaci\'{o}n de poses basado en landmarks. Un landmark es un punto en el espacio que representa una caracter\'{i}stica del sujeto al que se le est\'{a} extrayendo la pose para un frame dado. Mediapipe se divide en distintos estimadores de landmarks, ya que no solo detecta las poses corporales, si no que tambi\'{e}n estima landmarks en las facciones de la faciales de los sujetos entre otras, dando informaci\'{o}n muy precisa de los movimientos de los sujetos. Adicionalmente, Mediapipe tiene una API donde se pueden extraer esos puntos (landmarks). Esto es de especial conveniencia, debido a que, podemos utilizar esos landmarks como un vector de caracter\'{i}sticas que representa poses espaciotemporales para pas\'{a}rselo a una red basada en aprendizaje profundo y que aprenda patrones en las poses para poder clasificarlas.

\noindent 

\noindent \includegraphics*[width=4.76in, height=3.77in]{image4}

\noindent 

\noindent (A\~{n}adir al \'{i}ndice de figuras y referencia: https://github.com/AshishPandey88/Pose-Detection)

\noindent 

\noindent Mediapipe, en su versi\'{o}n hol\'{i}stica, eval\'{u}a al sujeto como un ``todo'' analizando no solo las poses, si no todo el conjunto caracter\'{i}sticas extra\'{i}bles del sujeto, como las expresiones faciales o los dedos y las palmas de las manos, caracter\'{i}sticas muy \'{u}tiles para la detecci\'{o}n de lengua de signos. Estas caracter\'{i}sticas, como se ha explicado previamente, pueden ser extra\'{i}das como un vector.

\noindent 

\noindent Por otro lado, Mediapipe ofrece dos versiones a la hora de realizar inferencias. La primera est\'{a} basada en CUDA(ref) y TensorRT(ref), librer\'{i}as propietarias de NVIDIA que facilitan las operaciones matriciales y la velocidad en la inferencia de modelos. La segunda, basada en tensorflow para CPU y modelos ligeros (lite) que permite usar mediapipe en cualquier tipo de dispositivo que tenga un int\'{e}rprete de Python con Tensorflow instalado en \'{e}l (Ordenadores sin GPU dedicada, Raspberry Pi, tel\'{e}fonos Android{\dots} etc.)

\noindent 

\noindent Estas utilidades hicieron que finalmente se utilizase Mediapipe a la hora de implementar uno de los varios modelos propuestos en este trabajo.


\paragraph{ Redes Convolucionales 3D (3D-CNNs)}

\noindent Este modelo es una extensi\'{o}n de las redes convolucionales 2D. Una red convolucional 2D es un modelo es capaz de capturar caracter\'{i}sticas espaciales. Esta caracter\'{i}stica se debe principalmente a la operaci\'{o}n principal que realiza el modelo, la convoluci\'{o}n. 

\noindent 

\noindent En un espacio continuo, una convoluci\'{o}n se define de la siguiente manera:

\noindent 

\noindent \includegraphics*[width=3.43in, height=0.50in]{image5}

\noindent 

\noindent (A\~{n}adir al \'{i}ndice de figuras y mencionar a Deep Learning Book puesto que esta f\'{o}rmula es extra\'{i}da de la secci\'{o}n 9.1 The convolution Operation'')

\noindent 

\noindent Matem\'{a}ticamente, una convoluci\'{o}n es una operaci\'{o}n matem\'{a}tica que realiza una transformaci\'{o}n de dos funciones (\textit{f }y \textit{g}) en una funci\'{o}n que representa una superposici\'{o}n de las caracter\'{i}sticas de ambas. Esta caracter\'{i}stica es de especial funcionalidad debido a que, en el tratamiento de se\~{n}ales, es capaz de combinar dos se\~{n}ales mezclando las caracter\'{i}sticas de ambas en una sola. 

\noindent 

\noindent Los colores de las im\'{a}genes o ``frames'' son codificados en 3 canales rojo, verde y azul (RGB). Estos tienen, por lo general 256 niveles (8 bits), siendo el 0 el nivel m\'{a}s bajo de intensidad de color y 255 el m\'{a}ximo. Si queremos realizar cualquier tipo de transformaci\'{o}n sobre dichos colores, necesitaremos otra se\~{n}al o funci\'{o}n. Dentro del mundo de tratamiento de im\'{a}genes, esa ``se\~{n}al'' adicional ser\'{a} lo que se denomina un kernel o filtro. Esto hace que podamos aplicar la funci\'{o}n vista previamente al espacio discreto:

\noindent 

\noindent \includegraphics*[width=3.62in, height=0.40in]{image6}

\noindent 

\noindent (A\~{n}adir al \'{i}ndice de figuras y mencionar a Deep Learning Book puesto que esta f\'{o}rmula es extra\'{i}da de la secci\'{o}n 9.1 The convolution Operation'')

\noindent 

\noindent Un kernel o filtro, puede ser considerado una matriz de n\'{u}meros cuya funci\'{o}n es aplicarse en un subconjunto de pixeles que forman un frame o imagen para poder extraer caracter\'{i}sticas del mismo. Una de las aplicaciones m\'{a}s generalizadas es la de extraer los ``edges'' o bordes de una imagen, como se puede ver en la siguiente imagen:

\noindent 

\noindent \includegraphics*[width=6.09in, height=1.54in]{image7}

\noindent 

\noindent (A\~{n}adir al \'{i}ndice de figuras y mencionar a 3Blue1Brown, dado que la imagen es de su video ``What is a Convolution?'')

\noindent 

\noindent En este caso, la convoluci\'{o}n pasar\'{i}a por multiplicar coordenada a coordenada los valores de cada uno de los pixeles a los valores del kernel y luego sumar cada uno de esos valores de la multiplicaci\'{o}n. Si los p\'{i}xeles a los que aplican el kernel tienen todos los mismos valores, el valor resultante ser\'{a} neutro (0) ya que la primera columna del kernel son todo valores negativos y la tercera todos positivos. En cambio, si los colores son diferentes (bordes de una superficie en la imagen), los valores obtenidos de la convoluci\'{o}n ser\'{a}n diferentes de 0, por lo que el valor ser\'{a} o positivo o negativo en el caso que el p\'{i}xel analizado forme parte de un borde vertical. Si se quisiesen detectar los bordes horizontales se deber\'{a} de aplicar una transposici\'{o}n al kernel, ya que los valores distintos de 0 se dar\'{i}an precisamente en aquellos bordes que marcan las l\'{i}neas horizontales. Una vez extra\'{i}das las caracter\'{i}sticas, estas servir\'{a}n a la red para aprender los distintos patrones asociados a las predicciones que nuestro modelo realizar\'{a}.

\noindent 

\noindent Una convoluci\'{o}n 3D, realiza el mismo proceso, pero a\~{n}adiendo una dimensi\'{o}n adicional, que puede ser o bien la profundidad de la imagen captura como es el caso de (ref a paper de action recognition UAH), ya que permite extraer caracter\'{i}sticas de entornos m\'{a}s complejos, o puede ser aplicado para v\'{i}deos, a\~{n}adiendo esa dimensi\'{o}n extra (temporal) y as\'{i} poder capturar las dependencias temporales.

\noindent 

\noindent \includegraphics*[width=6.09in, height=3.58in]{image8}

\noindent 

\noindent (ref https://www.tensorflow.org/tutorials/video/video\_classification y Figura)

\noindent 

\noindent Como se puede ver en la siguiente figura, a la izquierda tenemos una serie de matrices, donde cada matriz, aplicado a nuestro problema, puede ser un frame en un instante de tiempo t, y el conjunto de matrices, el v\'{i}deo en s\'{i}. Los kernels, o filtros, tendr\'{a}n ahora una dimensi\'{o}n extra, para poder capturar las dependencias temporales y extraer, no solo las caracter\'{i}sticas de un frame si no de varios a la vez, siendo estos secuenciales.

\noindent 


\paragraph{ Redes neuronales recurrentes: LSTM}

\noindent Las redes neuronales recurrentes (RNN) son un tipo de redes neuronales dise\~{n}adas para manejar datos secuenciales. Estas redes tienen ``memoria'', ya que la salida de un c\'{a}lculo en un instante t, afecta a la entrada del c\'{a}lculo de la salida para t+1, pudiendo as\'{i} procesar secuencias de datos a lo largo del tiempo. A esta memoria, se le suele denominar el estado oculto de la red, y es un vector que guarda informaci\'{o}n de los estados anteriores de la red para poder tenerlo en cuenta en futuras iteraciones. Una red neuronal tiene la siguiente forma:

\noindent 

\noindent \includegraphics*[width=2.48in, height=2.35in]{image9}\includegraphics*[width=2.93in, height=2.27in]{image10}

\noindent Izda (Refhttps://datascientest.com/es/wp-content/uploads/sites/7/2021/07/Sans-titre-1-Recupere-08.png) Dcha(referencia https://datascientest.com/es/recurrent-neural-network-rnn-de-que-se-trata) y meter como figura

\noindent 

\noindent Donde vemos que la red tiene una \'{u}nica entrada \textit{X${}_{t\ }$}que hace referencia a la entrada de los datos secuenciales \textit{X} en un instante de tiempo \textit{t}. En dicha c\'{e}lula se realizan las operaciones pertinentes, que depender\'{a}n del tipo de algoritmo que se implemente dentro de estas, detalle que veremos m\'{a}s adelante. La salida de la c\'{e}lula, $Y$\textit{${}_{t}$}, ser\'{a} por tanto el resultado de las computaciones internas de la c\'{e}lula, junto con el estado oculto que se va actualizando en cada instante de tiempo. El valor resultado en el instante t ser\'{a} la predicci\'{o}n del valor de \textit{Y${}_{t}$${}_{+1}$}

\noindent 

\noindent Si desenroll\'{a}semos la recursividad, tendr\'{i}amos un esquema similar al de la (figura de la derecha), donde vemos que cada c\'{e}lula 

\noindent 


\paragraph{ Subsubsecci\'{o}n}

\noindent 


\paragraph{ Subsubsecci\'{o}n}

\noindent 

\noindent 

\noindent 

\noindent \textbf{\includegraphics*[width=1.18in, height=1.03in]{image11}}

\noindent \textbf{Figura 2-1: Logo EPS}

\noindent 

\noindent 


\section{ Dise\~{n}o}


\subsection{ Subsecci\'{o}n}


\paragraph{ Subsubsecci\'{o}n}

\noindent 




\section{ Desarrollo}


\subsection{ Subsecci\'{o}n}


\paragraph{ Subsubsecci\'{o}n}

\noindent 

\noindent 


\section{ Integraci\'{o}n, pruebas y resultados}

\noindent 

\noindent 


\section{ Conclusiones y trabajo futuro}


\subsection{ Conclusiones}

\noindent 


\subsection{ Trabajo futuro}

\noindent 

\noindent 

\noindent Presupuesto

\noindent \underbar{}

\noindent \underbar{}

\noindent \underbar{}
\[11\] 

{\bf }

\noindent 
\section{Referencias}

\noindent 

\begin{enumerate}
\item  En las referencias figurar\'{a}n los autores (opcionalmebte los editors), el t\'{i}tulo del art\'{i}culo, el nombre de la revista o libro, el volumen y n\'{u}mero de la revista, las p\'{a}ginas del art\'{i}culo, la fecha de edici\'{o}n,. A continuaci\'{o}n se listan algunos ejemplos

\item  K.N. Platanioitis, C.S. Regazzoni (eds.), ``Special Issue in Visual-centric Surveillance Networks and Services'', IEEE Signal Processing Magazine, 22\eqref{GrindEQ__2_}, Marzo 2005.

\item  B.S. Manjunath, P. Salembier, T. Sikora (eds.), ``Introduction to MPEG 7: Multimedia Content Description Language,'', John Wiley and Sons, 2002

\item  G. R. Bradski, ``Computer vision face tracking as a component of a perceptual user interface,'' en Proc.IEEE Workshop on Applications of Computer Vision, Princeton, NJ, October 1998, pp. 214--219.

\item  A. D. Bue, D. Comaniciu, V. Ramesh, and C. Regazzoni, ``Smart cameras with real-time video object generation,'' in Proc. IEEE Intl. Conf. on Image Processing, Rochester, NY, volume III, 2002, pp. 429--432.

\item  P. Anandan. ``A computacional cuadrowork and an algorithm for the measurement of visual motion'', International Journal of Computer Vision, 2\eqref{GrindEQ__3_}:283-310, January, 1989.

\item  W.J. Ruckelidge. ``Efficient Computation of the minimum Hausdorff Distance for Visual Recognition'', Phd thesis, Cornell Universitym 1995. CS-TR1454

\item  ``Extensible Markup Language (XML) 1.0 (Second Edition)'', W3C Recommendation 6 October 2000 http://www.w3.org/TR/REC-xml\textbf{}

\item \textbf{ }William H. Press, Saul A.Teukolsky, William T. Vetterling, Brian P. Flannery. ``Numerical Recipes in C -- The art of Scientific Computing 2nd Edition''. Cambridge University Press\textbf{}
\end{enumerate}

\noindent 

\noindent 


\end{document}

